{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a3e202",
   "metadata": {},
   "source": [
    "# Trip Advisor Hotel Reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98305bc3",
   "metadata": {},
   "source": [
    "## Importing Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a599cbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from tqdm import trange\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a005c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Set the data path to the location of the downloaded data files\n",
    "nltk.data.path.append('C:\\\\Users\\\\susan/nltk_data')\n",
    "\n",
    "# Now you can use NLTK functions without encountering the error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2cf147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e0cbd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378da13c",
   "metadata": {},
   "source": [
    "## Importing the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9284ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love monaco staff husband stayed hotel crazy w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cozy stay rainy city, husband spent 7 nights m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>excellent staff, housekeeping quality hotel ch...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hotel stayed hotel monaco cruise, rooms genero...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>excellent stayed hotel monaco past w/e delight...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3\n",
       "3  unique, great stay, wonderful time hotel monac...       5\n",
       "4  great stay great stay, went seahawk game aweso...       5\n",
       "5  love monaco staff husband stayed hotel crazy w...       5\n",
       "6  cozy stay rainy city, husband spent 7 nights m...       5\n",
       "7  excellent staff, housekeeping quality hotel ch...       4\n",
       "8  hotel stayed hotel monaco cruise, rooms genero...       5\n",
       "9  excellent stayed hotel monaco past w/e delight...       5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('E:/1 task/tripadvisor_hotel_reviews.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63a7b1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    9054\n",
       "4    6039\n",
       "3    2184\n",
       "2    1793\n",
       "1    1421\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the number of ratings in the dataset\n",
    "\n",
    "data['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a7f2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Numberting the rating 4, 5 as Positive and 1, 2, 3 as Negative\n",
    "\n",
    "def ratings(rating):\n",
    "    if rating>3 and rating<=5:\n",
    "        return \"Positive\"\n",
    "    if rating>0 and rating<=3:\n",
    "        return \"Negative\"\n",
    "\n",
    "#Piechart\n",
    "data['Rating'] = data['Rating'].apply(ratings)\n",
    "plt.pie(data['Rating'].value_counts(), labels=data['Rating'].unique().tolist(), autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3e618",
   "metadata": {},
   "source": [
    "## Data Pre Processing\n",
    "\n",
    "Pre processing the data to remove noise and irrelevant information.\n",
    "\n",
    "The preprocessing includes: count, rating variable, removing punctuation, lowercase, stopwords, tokenization, stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced45af8",
   "metadata": {},
   "source": [
    "### Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7249bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for removing the punctuations\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9e000a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4 experience hotel monaco seatt...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique great stay wonderful time hotel monaco ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay went seahawk game awesom...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4 experience hotel monaco seatt...       3\n",
       "3  unique great stay wonderful time hotel monaco ...       5\n",
       "4  great stay great stay went seahawk game awesom...       5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Review']= data['Review'].apply(lambda x:remove_punctuation(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d1be4",
   "metadata": {},
   "source": [
    "### Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82008ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4 experience hotel monaco seatt...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique great stay wonderful time hotel monaco ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay went seahawk game awesom...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4 experience hotel monaco seatt...       3\n",
       "3  unique great stay wonderful time hotel monaco ...       5\n",
       "4  great stay great stay went seahawk game awesom...       5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Review']= data['Review'].apply(lambda x: x.lower())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7897d",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64100690",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Review']= data['Review'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62eec0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_review(review):\n",
    "    tokens = word_tokenize(review)\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2c4c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Set the data path to the location of the downloaded data files\n",
    "nltk.data.path.append('C:\\\\Users\\\\susan\\\\anaconda3\\\\lib\\\\nltk_data')\n",
    "\n",
    "# Now you can use NLTK functions without encountering the error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a2e086b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\susan/nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan/nltk_data'\n    - 'C:\\\\Users\\\\susan/nltk_data'\n    - 'C:\\\\Users\\\\susan/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\anaconda3\\\\lib\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_review\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36mtokenize_review\u001b[1;34m(review)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_review\u001b[39m(review):\n\u001b[1;32m----> 2\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\susan/nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan/nltk_data'\n    - 'C:\\\\Users\\\\susan/nltk_data'\n    - 'C:\\\\Users\\\\susan/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\susan\\\\anaconda3\\\\lib\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "data['tokens'] = data['Review'].apply(tokenize_review)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949483d1",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d15cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4f48eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for removing stopwords\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d465716a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokens'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:remove_stopwords(x))\n\u001b[0;32m      2\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokens'"
     ]
    }
   ],
   "source": [
    "data['tokens']= data['tokens'].apply(lambda x:remove_stopwords(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dabceec",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "<p>It is also known as the text standardization step where the words are stemmed or diminished to their root/base form.</p>  \n",
    "<p>For example, words like ‘programmer’, ‘programming, ‘program’ will be stemmed to ‘program’.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e364c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "def stemming(text):\n",
    "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b5640e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokens'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstem\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: stemming(x))\n\u001b[0;32m      2\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokens'"
     ]
    }
   ],
   "source": [
    "data['stem']=data['tokens'].apply(lambda x: stemming(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e66ba5",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b902c3",
   "metadata": {},
   "source": [
    "### Word frequency Counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11166e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "for i in df['lemmatize']:\n",
    "    for j in i:\n",
    "        if j not in freq:\n",
    "            freq[j] = 1\n",
    "        else:\n",
    "            freq[j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94001b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(freq.items())[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a8d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df=pd.DataFrame(sorted(freq.items(),key=lambda x:x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cfa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=freq_df[0][:10]\n",
    "f2=freq_df[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "645a95e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m1\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(\u001b[43mf1\u001b[49m,f2,color \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m,width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords in the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f1' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This gives the most frequently used words\n",
    "\n",
    "\n",
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(f1,f2,color ='blue',width = 0.4)\n",
    "plt.xlabel(\"Words in the dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 words in the dataframe\")\n",
    "plt.savefig(\"wordfrequency.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0a56c",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "\n",
    "In natural language processing (NLP), an n-gram is a contiguous sequence of n items from a given sequence of text. These items can be characters, words, or even sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24210032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_grams(text,n=1):\n",
    "  my_ngrams = ngrams(text.split(), n)\n",
    "  return my_ngrams\n",
    "\n",
    "# We can change the value of 'n' for the corresponding n-gram analysis we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = defaultdict(int)\n",
    "for text in df.Review:\n",
    "  for word in generate_N_grams(text,2):\n",
    "    i = word[0]+' '+word[1]\n",
    "    ngram[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram=pd.DataFrame(sorted(ngram.items(),key=lambda x:x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3bcb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=ngram[0][:10]\n",
    "d2=ngram[1][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2789f896",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m1\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(\u001b[43md1\u001b[49m,d2,color \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m,width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords in the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd1' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(d1,d2,color ='green',width = 0.4)\n",
    "plt.xlabel(\"Words in the dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 words in the dataframe-BIGRAM ANALYSIS\")\n",
    "plt.savefig(\"bigram_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# We should remove the stopwords for better analysis of the n-grams appart from \"did'nt\" and 'did not', \n",
    "#'great location' and 'staff friendly' are the most used ngrams in the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c70f19",
   "metadata": {},
   "source": [
    "### Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3530c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c9651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
